# ü¶æ SimulInf App (WIP)
Simulate inference performances of your AI model on different hardware and cloud platforms.

If you like this App, give us a star to show your support for the project ‚≠ê

## üìñ Description
SimulInf is a powerful tool to simulate the inference performance of deep learning models on different hardware. It allows users to easily select the fastest or most cost-effective hardware for their model, without the need for expensive trial and error.

With SimulInf, users can choose from a list of pre-defined models or input their own model in their preferred deep learning framework. The App simulates the inference performance of the model on a range of hardware, including CPUs from Intel and AMD, GPUs from Nvidia and AMD, and TPUs.

Users can compare the results side-by-side, assessing response time and estimated inference cost of the model on each platform. This allows users to make informed decisions about which hardware is best suited for their needs and budget. The App can also estimate the cost per inference on different cloud services, making it easy to compare the costs of running the model on different platforms.

Overall, SimulInf provides an easy-to-use and accurate way to simulate the inference of AI models on different hardware and cloud platforms. It helps users speed up development times,  making the most of their budgets and optimizing for superior inference performances. Try it out today, and reach out if you have any feedback!